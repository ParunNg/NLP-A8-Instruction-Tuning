{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5896b1a6-8ed4-4484-96e5-50d0ac10b73a",
   "metadata": {},
   "source": [
    "# [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)\n",
    "\n",
    "Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "[Python Script](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f636b5-91b3-4a45-a6cf-334425eac4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install peft==0.7.1\n",
    "# !pip3 install trl==0.7.4\n",
    "# !pip3 install transformers==4.36.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd24274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7263f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.7.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ed1948-2b9b-4324-ba26-36b6c95fdbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16f67f1-c0e4-40e9-b192-4d1a9cfbfb17",
   "metadata": {},
   "source": [
    "## Instruction-Tuning\n",
    "Train on completions only\n",
    "- Use the DataCollatorForCompletionOnlyLM to train your model on the generated prompts only.\n",
    "- Note that this works only in the case when packing=False.\n",
    "- To instantiate that collator for instruction data, pass a response template and the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1ea5ec-482c-4520-bd97-3ccc1f2961f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'instruction', 'output'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "from datasets import load_dataset\n",
    "train_ds = load_dataset('json', data_files='dataset/alpaca_data.json', split='train')\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6ee677-ddc8-4a81-9d86-9219de3d84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '(A musical note)',\n",
       " 'instruction': 'Name the given musical note.',\n",
       " 'output': 'The musical note is an F sharp.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9b7c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output'],\n",
       "    num_rows: 805\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds = load_dataset(\"tatsu-lab/alpaca_eval\", split='eval', trust_remote_code=True)\n",
    "eval_ds = eval_ds.remove_columns([\"generator\", \"dataset\"])\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8433703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'what are five important topics for game design',\n",
       " 'output': '1. Storytelling\\n2. Player Mechanics\\n3. Art Direction\\n4. Level Design\\n5. User Interface Design'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69100196-d9d8-4791-9e11-6e93f1bd7550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load the model & Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"distilgpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map = 'auto')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Make sure to pass a correct value for max_seq_length as the default value will be set to min(tokenizer.model_max_length, 1024).\n",
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75210c9",
   "metadata": {},
   "source": [
    "### Standard-Alpaca : Format your input prompts\n",
    "For instruction fine-tuning, it is quite common to have two columns inside the dataset: one for the prompt & the other for the response.\n",
    "\n",
    "This allows people to format examples like Stanford-Alpaca did as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d350a2-002b-40e2-8c10-9afea5923cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "\toutput_texts = []\n",
    "\n",
    "\tfor i in range(len(examples['instruction'])):\n",
    "\t\tinstruction = examples[\"instruction\"][i]\n",
    "\t\tinput_text = examples[\"input\"][i] if 'input' in examples.keys() else \"\"\n",
    "\t\tresponse = examples[\"output\"][i]\n",
    "\t\n",
    "\t\tif len(input_text) > 1:\n",
    "\t\t\ttext = f\"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "\"\"\".strip()\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\ttext = f\"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "\"\"\".strip()\n",
    "\n",
    "\t\toutput_texts.append(text)\n",
    "\n",
    "\treturn output_texts\n",
    "\n",
    "#check instruction-prompt\n",
    "formatting_prompts_func(train_ds[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44cf1dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForCompletionOnlyLM(tokenizer=GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the DataCollatorForCompletionOnlyLM to train your model on the generated prompts only\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"### Response:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f883e0a",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28385087-8eb8-4b83-a7dd-1313bf591b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3750 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 1250/3750 [02:37<03:50, 10.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5394, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|███▎      | 1250/3750 [02:57<03:50, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1940672397613525, 'eval_runtime': 19.9782, 'eval_samples_per_second': 40.294, 'eval_steps_per_second': 5.056, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2500/3750 [05:37<01:54, 10.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2647, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 2500/3750 [05:57<01:54, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1724660396575928, 'eval_runtime': 19.9961, 'eval_samples_per_second': 40.258, 'eval_steps_per_second': 5.051, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [08:36<00:00, 10.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1411, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 3750/3750 [08:56<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.168952703475952, 'eval_runtime': 19.9561, 'eval_samples_per_second': 40.338, 'eval_steps_per_second': 5.061, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [08:57<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 537.7753, 'train_samples_per_second': 55.785, 'train_steps_per_second': 6.973, 'train_loss': 2.3150337890625, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "save_path = './results/final'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results', #default = 'tmp_trainer'\n",
    "    save_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 3, #default = 3\n",
    ")\n",
    "\n",
    "# Step 3: Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds.select(range(10000)),\n",
    "    eval_dataset = eval_ds,\n",
    "    formatting_func = formatting_prompts_func,\n",
    "    data_collator = collator,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9e834",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5da45651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "save_path = './results/final'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_path,\n",
    "    device_map = 'auto')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    save_path)\n",
    "\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map = 'auto',\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 100,\n",
    "    temperature = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a07a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_prompt(sample):\n",
    "\t\n",
    "\tif 'input' in sample.keys():\n",
    "\t\treturn f\"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\t\t\t\n",
    "\telse:\n",
    "\t\treturn f\"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96f3fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def compare_responses(pipeline, sample):\n",
    "    print(f\"Instruction:\\n{sample['instruction']}\\n\")\n",
    "\n",
    "    if 'input' in sample.keys():\n",
    "        if len(sample['input']) > 1:\n",
    "            print(f\"Input:\\n{sample['input']}\\n\")\n",
    "\n",
    "    print(f\"Gold Response:\\n{sample['output']}\\n\")\n",
    "\n",
    "    output = pipeline(instruction_prompt(sample))\n",
    "    response = output[0]['generated_text'].split(\"### Response:\\n\")[-1]\n",
    "\n",
    "    print(f\"Generated Response:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a0cea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "do you think retinoid is effective on removing the acne? because I have a lot of it\n",
      "\n",
      "Gold Response:\n",
      "Yes, retinoids are effective in treating acne. They work by increasing cell turnover, which helps to reduce the appearance of existing acne and prevent new outbreaks. Retinoids also help to unclog pores, which in turn reduces the amount of bacteria that can cause infections. In general, retinoids help to reduce inflammation and oil production, making them a great option for those with acne.\n",
      "\n",
      "Generated Response:\n",
      "Yes, retinoid can help reduce acne by eliminating symptoms like bloating. Just use products like soap and makeup to help with aging. Additionally, it can boost your risk of getting acne, leading to acne. Finally, it has great antioxidant properties and anti-aging, so it's absolutely worthwhile to try retinoid before you make your next acne!\n",
      "I also recommend the antihydrogen gel Retinoid in the form of moisturizers. I love it and love how it keeps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_responses(text_generator, eval_ds[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
